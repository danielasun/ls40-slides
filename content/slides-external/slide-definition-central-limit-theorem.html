
<section>
  <h3>Central limit theorem</h3>
  <div class="smaller">
    The Central Limit Theorem is one of the most fundamental results in probability theory.
    In its general form, this theorem establishes that:
  </div>
  <div class="smaller quoted down-2">
    For <span class="danger">independant random samples</span> with a <span class="u-danger">sufficiently large sample size</span>, the distribution of <span class="pop">many common sample statistics</span> can be <span class="danger">approximated</span> with a <span class="u-danger">normal distribution centered at the population parameter</span> and with standard deviation equal to the standard error of the sample statistics. This will hold true regardless of whether the source population of the random sample is discrete or continuous, and normal or skewed, provided the sample size is "sufficiently large".
  </div>
  <div class="smaller down-4 framed border-muted">
    <span class="muted u em">Note:</span> The necessary sample size (<span class="em">n</span>) to be considered "sufficiently large" depends on the skewness of the distribution from which the random sample is drawn from:
    <ul class="normal">
      <li>
        If the distribution of the random sample is symmetric, unimodal or continuous, then a sample size <span class="em">n</span> as small as 4 or 5 yields an adequate approximation.
      </li>
      <li>
        If the distribution of the random sample is skewed, then a sample size <span class="em">n</span> of at least 25 or 30 yields an adequate approximation.
      </li>
      <li>
        If the distribution of the random sample is extremely skewed, then you may need an even larger <span class="em">n</span>.
      </li>
    </ul>    
  </div>
</section>




