<section>
  <h3>Linear regression analysis</h3>
  <p>
    Linear regression analysis consists of <span class="info">more than
    just fitting a linear line</span> through a cloud of data points. The linear regression analysis of a dataset can be decomposed in <span class="info">3 stages</span>:
  </p>
  <ol class="small down-3">
    <li>
      <span class="u strong">Analyzing</span> the correlation and directionality of the data (checking if linear regression is an <span class="danger">appropriate model</span> for the data you want to analyze)
    </li>
    <li>
      <span class="u strong">Estimating</span> the model using the data (fitting the line, estimating the <span class="danger">best parameters</span> for the line equation)
    </li>
    <li>
      <span class="u strong">Evaluating</span> the <span class="danger">validity and usefulness</span> of the model
    </li>
  </ol>
</section>

<section>
  <h3>Evaluating the model</h3>
  <p class="smaller">
    Three statistics are commonly used in linear regression analysis to evaluate how well the model fit the data:
  </p>
  <ul class="small">
    <li>
      The <span class="pop">$R^2$</span>, or <span class="em strong">coefficient of determination</span>
    </li>
    <li>
      The <span class="pop">$RMSE$</span>, or <span class="em strong">Root Mean Square Error</span>
    </li>
    <li>
      The <span class="orange">$F$-statistic</span> (we will talk about it later in class, when we will compare more than two quantitative variables and perform ANOVA)
    </li>
  </ul>
  <p class="small down-2 em u">
    All three statistics are based on two sums of squares:
  </p>
  <table class="smaller down-2">
    <col width=50%>
    <col width=50%>
    <tbody>
      <tr>
        <td class="bg-info-light">Total Sum of Squares ($TSS$)</td>
        <td class="bg-danger-light">Sum of Squares Error ($SSE$)</td>
      </tr>
      <tr class="border-bottom">
        <td>
          it measures how far the response variable data are from the mean.
          $TSS=\sum_{i=1}^{n}(y-\texclass{info}{\bar{y}})^2$
        </td>
        <td>
          it measures how far the data are from the model’s predicted values.
          $SSE=\sum_{i=1}^{n}(y-\texclass{danger}{\hat{y}})^2$
        </td>
      </tr>
    </tbody>
  </table>
  <p class="small">
    <i class="fas fa-arrow-circle-right"></i>
     Different combinations of these two values provide different information about how the regression model compares to the mean model.
  </p>
</section>


<section>
  <h3>Coefficient of determination, $R^2$</h3>
  <p class="framed border-pink">
    $R^2$ measures the percentage of total variation
    in the response variable that is explained by the linear relationship.
  </p>
  <p>
    <span class="pink">$R^2=1-\frac{\textrm{Sum of squared errors (SSE)}}{\textrm{Total sum of squares (TSS)}}$</span>
    $= 1-\frac{\sum_{i=1}^{n}(y-\texclass{danger}{\hat{y}})^2}{\sum_{i=1}^{n}(y-\texclass{info}{\bar{y}})^2}$
  </p>
  <p class="u em muted down-3">
    Notes:
  </p>
  <ul class="small">
    <li> 
      Another way to compute $R^2$ is by litteraly calculating the <span class="orange">square of the correlation coefficient</span> ($r$) <i class="fas fa-arrow-right"></i> $R^2 = r^2$
    </li>
    <li>
      The slope ($m$) of the regresion line tells you how much change in $Y$ you can expect based on a given change in $X$. The coefficient $R^2$ tells you <span class="orange strong">how much of the variation in $Y$ is attributable to (explained by) the variation in $X$</span>. The rest of the variation (or <span class="em danger">“noise”</span>), is accounted by other factors, such as measurement error, individual variation, etc.
    </li>
  </ul>
</section>

<section>
  <h3>Root mean square error, $RMSE$</h3>
  <p class="framed border-pink">
    The $RMSE$ measures how close the observed data points are to the model’s predicted values.
  </p>
  <p>
    The $RMSE$ is a common metric to evaluate model performance and computes the square root of the average squared residual (distance from the prediction to the best fit).
  </p>
  <p class="pink">
    $RMSE=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y-\hat{y})^2}$
  </p>
  <p class="smaller down-3">
    <i class="fas fa-arrow-circle-right orange"></i>
    The key idea behind the computation of the $RMSE$
    is that predictions ($\hat{y}$) that are <span class="orange">far away from the true observed data</span>
    value ($y$) will contribute more heavily to <span class="orange">increasing the value of the $RMSE$</span> than predictions that are close to the true values.
  </p>
  <p class="down-2">
    <i class="fas fa-arrow-circle-right orange"></i>
    <span class="orange">Lower</span> values of RMSE indicate <span class="orange">better</span> fit.
  </p>
</section>

<section>
  <h3>Model evaluation, $R^2$ vs $RMSE$</h3>
  <table class="smaller">
    <col width=50%>
    <col width=50%>
    <tbody>
      <tr>
        <td class="bg-info-light center">$R^2$</td>
        <td class="bg-danger-light center">$RMSE$</td>
      </tr>
      <tr>
        <td>Square of the correlation coefficient</td>
        <td>Square root of the average squared residual</td>
      </tr>
      <tr>
        <td colspan="2" class="center">Both give an indication about the <span class="em">"goodness of the fit"</span></td>
      </tr>
      <tr>
        <td>$R^2$ is conveniently scaled between 0 and 1 (<span class="info">relative measure</span> of fit)</td>
        <td>$RMSE$ is not scaled to any particular value (<span class="danger">absolute measure</span> of fit)</td>
      </tr>
      <tr>
        <td>$R^2$ can be more easily interpreted
          in the <span class="info">context of the linear association</span>
          analyzed
          (how successful the fit is in explaining the variation of the data)
        </td>
        <td>$RMSE$ explicitly provides information about <span class="danger">how much our predictions deviate, on average</span>, from the actual values in the dataset</td>
      </tr>
      <tr>
        <td>Its main purpose is either the <span class="info">prediction</span> of future outcomes or the <span class="info">testing</span> of hypotheses.</td>
        <td>
          It is one of the most important criterion for fit if the main purpose of the model is <span class="danger">prediction</span>
        </td>
      </tr>
    </tbody>
  </table>
</section>